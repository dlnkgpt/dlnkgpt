nohup: ignoring input
========================================================================
Starting AutoTrain for dLNk GPT Uncensored Model
========================================================================

Configuration:
  Project: dlnkgpt-uncensored
  Base Model: EleutherAI/gpt-j-6b
  Dataset: /home/ubuntu/dlnkgpt/model_finetuning/autotrain_dataset
  Output: /home/ubuntu/dlnkgpt/model_finetuning/autotrain_output

========================================================================
Launching AutoTrain...
========================================================================

INFO     | 2025-11-12 15:34:34 | autotrain.cli.run_llm:run:136 - Running LLM
WARNING  | 2025-11-12 15:34:34 | autotrain.trainers.common:__init__:286 - Parameters supplied but not used: version, func, backend, deploy, inference, config, train
Saving the dataset (0/1 shards):   0%|          | 0/54000 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 54000/54000 [00:00<00:00, 3804804.73 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 54000/54000 [00:00<00:00, 3756404.61 examples/s]
Saving the dataset (0/1 shards):   0%|          | 0/54000 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 54000/54000 [00:00<00:00, 4007864.10 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 54000/54000 [00:00<00:00, 3964995.12 examples/s]
INFO     | 2025-11-12 15:34:34 | autotrain.backends.local:create:20 - Starting local training...
WARNING  | 2025-11-12 15:34:34 | autotrain.commands:get_accelerate_command:58 - No GPU found. Forcing training on CPU. This will be super slow!
INFO     | 2025-11-12 15:34:34 | autotrain.commands:launch_command:514 - ['accelerate', 'launch', '--cpu', '-m', 'autotrain.trainers.clm', '--training_config', 'dlnkgpt-uncensored/training_params.json']
INFO     | 2025-11-12 15:34:34 | autotrain.commands:launch_command:515 - {'model': 'EleutherAI/gpt-j-6b', 'project_name': 'dlnkgpt-uncensored', 'data_path': 'dlnkgpt-uncensored/autotrain-data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': True, 'block_size': 512, 'model_max_length': 2048, 'padding': 'right', 'trainer': 'default', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': 100, 'eval_strategy': 'steps', 'save_total_limit': 2, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 2e-05, 'epochs': 3, 'batch_size': 4, 'warmup_ratio': 0.1, 'gradient_accumulation': 8, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'autotrain_prompt', 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'push_to_hub': True, 'username': 'dlnkgpt', 'token': '*****', 'unsloth': False, 'distributed_backend': None}
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `0`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
INFO     | 2025-11-12 15:35:00 | autotrain.trainers.clm.train_clm_default:train:26 - Starting default/generic CLM training...
INFO     | 2025-11-12 15:35:00 | autotrain.trainers.clm.utils:process_input_data:487 - loading dataset from disk
INFO     | 2025-11-12 15:35:00 | autotrain.trainers.clm.utils:process_input_data:550 - Train data: Dataset({
    features: ['autotrain_text'],
    num_rows: 54000
})
INFO     | 2025-11-12 15:35:00 | autotrain.trainers.clm.utils:process_input_data:551 - Valid data: None
INFO     | 2025-11-12 15:35:05 | autotrain.trainers.clm.utils:configure_logging_steps:671 - configuring logging steps
INFO     | 2025-11-12 15:35:05 | autotrain.trainers.clm.utils:configure_logging_steps:684 - Logging steps: 100
INFO     | 2025-11-12 15:35:05 | autotrain.trainers.clm.utils:configure_training_args:723 - configuring training args
INFO     | 2025-11-12 15:35:05 | autotrain.trainers.clm.utils:configure_block_size:801 - Using block size 512
INFO     | 2025-11-12 15:35:06 | autotrain.trainers.clm.utils:get_model:877 - Can use unsloth: False
WARNING  | 2025-11-12 15:35:06 | autotrain.trainers.clm.utils:get_model:919 - Unsloth not available, continuing without it...
INFO     | 2025-11-12 15:35:06 | autotrain.trainers.clm.utils:get_model:921 - loading model config...
INFO     | 2025-11-12 15:35:07 | autotrain.trainers.clm.utils:get_model:929 - loading model...
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
ERROR    | 2025-11-12 15:35:07 | autotrain.trainers.common:wrapper:215 - train has failed due to an exception: Traceback (most recent call last):
  File "/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py", line 1817, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1206, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1178, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1149, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/usr/local/lib/python3.11/dist-packages/transformers/integrations/bitsandbytes.py", line 21, in <module>
    import bitsandbytes as bnb
  File "/usr/local/lib/python3.11/dist-packages/bitsandbytes/__init__.py", line 15, in <module>
    from .nn import modules
  File "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/__init__.py", line 21, in <module>
    from .triton_based_modules import (
  File "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/triton_based_modules.py", line 7, in <module>
    from bitsandbytes.triton.int8_matmul_mixed_dequantize import (
  File "/usr/local/lib/python3.11/dist-packages/bitsandbytes/triton/int8_matmul_mixed_dequantize.py", line 12, in <module>
    from triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time
ModuleNotFoundError: No module named 'triton.ops'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.11/dist-packages/autotrain/trainers/common.py", line 212, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/autotrain/trainers/clm/__main__.py", line 23, in train
    train_default(config)
  File "/usr/local/lib/python3.11/dist-packages/autotrain/trainers/clm/train_clm_default.py", line 50, in train
    model = utils.get_model(config, tokenizer)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/autotrain/trainers/clm/utils.py", line 943, in get_model
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py", line 3620, in from_pretrained
    hf_quantizer.validate_environment(
  File "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 79, in validate_environment
    from ..integrations import validate_bnb_backend_availability
  File "<frozen importlib._bootstrap>", line 1231, in _handle_fromlist
  File "/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py", line 1805, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py", line 1819, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):
No module named 'triton.ops'

ERROR    | 2025-11-12 15:35:07 | autotrain.trainers.common:wrapper:216 - Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):
No module named 'triton.ops'
INFO     | 2025-11-12 15:35:10 | autotrain.cli.run_llm:run:141 - Job ID: 5498

========================================================================
Training Complete!
========================================================================
Model saved to: /home/ubuntu/dlnkgpt/model_finetuning/autotrain_output
Model also pushed to: https://huggingface.co/dlnkgpt/dlnkgpt-uncensored
========================================================================
