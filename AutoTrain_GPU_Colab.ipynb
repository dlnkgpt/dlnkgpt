{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# dLNk GPT Uncensored - AutoTrain on GPU\n",
        "\n",
        "This notebook trains the dLNk GPT uncensored model using Hugging Face AutoTrain with GPU acceleration.\n",
        "\n",
        "**Requirements:**\n",
        "- GPU Runtime (T4, A100, or V100)\n",
        "- Hugging Face Token\n",
        "- 12-16 hours training time\n",
        "\n",
        "**Steps:**\n",
        "1. Enable GPU: Runtime → Change runtime type → GPU\n",
        "2. Run all cells in order\n",
        "3. Monitor training progress"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages with compatible versions\n",
        "!pip install -q --upgrade huggingface_hub\n",
        "!pip install -q transformers accelerate bitsandbytes peft datasets\n",
        "!pip install -q autotrain-advanced"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Login to Hugging Face"
      ],
      "metadata": {
        "id": "login"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Enter your Hugging Face token\n",
        "HF_TOKEN = \"\"  # Replace with your token\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "print(\"Logged in to Hugging Face\")"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Dataset"
      ],
      "metadata": {
        "id": "load_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from Hugging Face Hub\n",
        "dataset = load_dataset(\"dlnkgpt/dlnkgpt-uncensored-dataset\")\n",
        "\n",
        "print(f\"Dataset loaded:\")\n",
        "print(f\"  Train: {len(dataset['train']):,} examples\")\n",
        "print(f\"  Validation: {len(dataset['validation']):,} examples\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample:\")\n",
        "print(dataset['train'][0]['text'][:200] + \"...\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare Dataset for Training"
      ],
      "metadata": {
        "id": "prepare"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save dataset locally\n",
        "dataset.save_to_disk(\"./autotrain_data\")\n",
        "print(\"Dataset saved locally\")"
      ],
      "metadata": {
        "id": "save_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Configure Training Parameters"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set environment variable\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "\n",
        "# Training configuration\n",
        "PROJECT_NAME = \"dlnkgpt-uncensored\"\n",
        "MODEL_NAME = \"EleutherAI/gpt-j-6b\"\n",
        "DATA_PATH = \"./autotrain_data\"\n",
        "TEXT_COLUMN = \"text\"\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_RATIO = 0.1\n",
        "GRADIENT_ACCUMULATION = 8\n",
        "BLOCK_SIZE = 512\n",
        "\n",
        "# LoRA parameters\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  LoRA Enabled: True\")"
      ],
      "metadata": {
        "id": "configure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Start Training\n",
        "\n",
        "**Note:** This will take 12-16 hours on T4 GPU. You can close the browser and come back later."
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch AutoTrain\n",
        "!autotrain llm \\\n",
        "  --train \\\n",
        "  --project-name {PROJECT_NAME} \\\n",
        "  --model {MODEL_NAME} \\\n",
        "  --data-path {DATA_PATH} \\\n",
        "  --text-column {TEXT_COLUMN} \\\n",
        "  --lr {LEARNING_RATE} \\\n",
        "  --epochs {EPOCHS} \\\n",
        "  --batch-size {BATCH_SIZE} \\\n",
        "  --warmup-ratio {WARMUP_RATIO} \\\n",
        "  --gradient-accumulation {GRADIENT_ACCUMULATION} \\\n",
        "  --block_size {BLOCK_SIZE} \\\n",
        "  --logging-steps 100 \\\n",
        "  --eval-strategy steps \\\n",
        "  --save-total-limit 2 \\\n",
        "  --peft \\\n",
        "  --lora-r {LORA_R} \\\n",
        "  --lora-alpha {LORA_ALPHA} \\\n",
        "  --lora-dropout {LORA_DROPOUT} \\\n",
        "  --mixed-precision fp16 \\\n",
        "  --push-to-hub \\\n",
        "  --username dlnkgpt \\\n",
        "  --token $HF_TOKEN"
      ],
      "metadata": {
        "id": "start_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Test Trained Model"
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Load base model\n",
        "print(\"Loading base model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-j-6b\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "print(\"Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"dlnkgpt/dlnkgpt-uncensored\"\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dlnkgpt/dlnkgpt-uncensored\")\n",
        "\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "# Test generation\n",
        "def generate_response(prompt, max_length=200):\n",
        "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Explain artificial intelligence\",\n",
        "    \"What is machine learning?\",\n",
        "    \"Describe neural networks\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting model...\")\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nTest {i}: {prompt}\")\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "**Training Complete!**\n",
        "\n",
        "Your model has been:\n",
        "- Trained on 54,000 examples\n",
        "- Fine-tuned for 3 epochs\n",
        "- Pushed to Hugging Face Hub\n",
        "- Ready to use\n",
        "\n",
        "**Model URL:** https://huggingface.co/dlnkgpt/dlnkgpt-uncensored\n",
        "\n",
        "**Next Steps:**\n",
        "1. Test the model with various prompts\n",
        "2. Integrate with your API\n",
        "3. Deploy to production"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
