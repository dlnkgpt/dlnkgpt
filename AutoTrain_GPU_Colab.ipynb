{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# dLNk GPT Uncensored - AutoTrain on GPU\n",
        "\n",
        "This notebook trains the dLNk GPT uncensored model using Hugging Face AutoTrain with GPU acceleration.\n",
        "\n",
        "**Requirements:**\n        "- GPU Runtime (T4, A100, or V100)\n",
        "- Hugging Face Token\n",
        "- ~12-16 hours training time\n",
        "\n",
        "**Steps:**\n",
        "1. Enable GPU: Runtime → Change runtime type → GPU\n",
        "2. Run all cells in order\n",
        "3. Monitor training progress"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q autotrain-advanced huggingface_hub datasets transformers accelerate bitsandbytes peft"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Login to Hugging Face"
      ],
      "metadata": {
        "id": "login"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Enter your Hugging Face token\n",
        "HHF_TOKEN = ""  # Enter your Hugging Face token heren\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "print(\"✓ Logged in to Hugging Face\")"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Dataset"
      ],
      "metadata": {
        "id": "load_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from Hugging Face Hub\n",
        "dataset = load_dataset(\"dlnkgpt/dlnkgpt-uncensored-dataset\")\n",
        "\n",
        "print(f\"✓ Dataset loaded:\")\n",
        "print(f\"  Train: {len(dataset['train']):,} examples\")\n",
        "print(f\"  Validation: {len(dataset['validation']):,} examples\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample:\")\n",
        "print(dataset['train'][0]['text'][:200] + \"...\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Configure Training"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Training configuration\n",
        "config = {\n",
        "    \"project_name\": \"dlnkgpt-uncensored\",\n",
        "    \"model\": \"EleutherAI/gpt-j-6b\",\n",
        "    \"dataset\": \"dlnkgpt/dlnkgpt-uncensored-dataset\",\n",
        "    \"text_column\": \"text\",\n",
        "    \"epochs\": 3,\n",
        "    \"batch_size\": 4,\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \"gradient_accumulation\": 8,\n",
        "    \"block_size\": 512,\n",
        "    \"use_peft\": True,\n",
        "    \"lora_r\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"fp16\": True,\n",
        "    \"push_to_hub\": True\n",
        "}\n",
        "\n",
        "# Set environment variable\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "\n",
        "print(\"✓ Configuration set\")\n",
        "print(f\"  Base Model: {config['model']}\")\n",
        "print(f\"  Epochs: {config['epochs']}\")\n",
        "print(f\"  Batch Size: {config['batch_size']} (effective: {config['batch_size'] * config['gradient_accumulation']})\")\n",
        "print(f\"  Learning Rate: {config['learning_rate']}\")\n",
        "print(f\"  PEFT/LoRA: {config['use_peft']}\")"
      ],
      "metadata": {
        "id": "configure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Start Training\n",
        "\n",
        "**Note:** This will take 12-16 hours on T4 GPU. You can close the browser and come back later."
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset locally first\n",
        "dataset.save_to_disk(\"./autotrain_data\")\n",
        "print(\"✓ Dataset saved locally\")"
      ],
      "metadata": {
        "id": "save_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch AutoTrain\n",
        "!autotrain llm \\\n",
        "  --train \\\n",
        "  --project-name {config['project_name']} \\\n",
        "  --model {config['model']} \\\n",
        "  --data-path ./autotrain_data \\\n",
        "  --text-column {config['text_column']} \\\n",
        "  --lr {config['learning_rate']} \\\n",
        "  --epochs {config['epochs']} \\\n",
        "  --batch-size {config['batch_size']} \\\n",
        "  --warmup-ratio {config['warmup_ratio']} \\\n",
        "  --gradient-accumulation {config['gradient_accumulation']} \\\n",
        "  --block_size {config['block_size']} \\\n",
        "  --logging-steps 100 \\\n",
        "  --eval-strategy steps \\\n",
        "  --save-total-limit 2 \\\n",
        "  --peft \\\n",
        "  --lora-r {config['lora_r']} \\\n",
        "  --lora-alpha {config['lora_alpha']} \\\n",
        "  --lora-dropout {config['lora_dropout']} \\\n",
        "  --mixed-precision fp16 \\\n",
        "  --push-to-hub \\\n",
        "  --username dlnkgpt \\\n",
        "  --token $HF_TOKEN"
      ],
      "metadata": {
        "id": "start_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Monitor Training (Optional)"
      ],
      "metadata": {
        "id": "monitor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check training logs\n",
        "!tail -50 {config['project_name']}/training.log"
      ],
      "metadata": {
        "id": "check_logs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Test Trained Model"
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Load base model\n",
        "print(\"Loading base model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-j-6b\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "print(\"Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    f\"dlnkgpt/{config['project_name']}\"\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"dlnkgpt/{config['project_name']}\")\n",
        "\n",
        "print(\"✓ Model loaded successfully\")\n",
        "\n",
        "# Test generation\n",
        "def generate_response(prompt, max_length=200):\n",
        "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Explain artificial intelligence in simple terms.\",\n",
        "    \"What is machine learning?\",\n",
        "    \"Describe neural networks.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Testing Trained Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n[Test {i}] Prompt: {prompt}\")\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\"*70)"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Download Model (Optional)"
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the model for download\n",
        "!zip -r dlnkgpt-uncensored.zip {config['project_name']}\n",
        "\n",
        "from google.colab import files\n",
        "files.download('dlnkgpt-uncensored.zip')\n",
        "\n",
        "print(\"✓ Model downloaded\")"
      ],
      "metadata": {
        "id": "download_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "**Training Complete!**\n",
        "\n",
        "Your model has been:\n",
        "- ✅ Trained on 54,000 examples\n",
        "- ✅ Fine-tuned for 3 epochs\n",
        "- ✅ Pushed to Hugging Face Hub\n",
        "- ✅ Ready to use\n",
        "\n",
        "**Model URL:** https://huggingface.co/dlnkgpt/dlnkgpt-uncensored\n",
        "\n",
        "**Next Steps:**\n",
        "1. Test the model with various prompts\n",
        "2. Integrate with your API\n",
        "3. Deploy to production\n",
        "\n",
        "**Usage:**\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\")\n",
        "model = PeftModel.from_pretrained(base_model, \"dlnkgpt/dlnkgpt-uncensored\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dlnkgpt/dlnkgpt-uncensored\")\n",
        "```"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
