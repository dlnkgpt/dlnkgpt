{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# dLNk GPT Uncensored - AutoTrain on GPU\n",
        "\n",
        "This notebook trains the dLNk GPT uncensored model using Hugging Face AutoTrain with GPU acceleration.\n",
        "\n",
        "**Requirements:**\n",
        "- GPU Runtime (T4, A100, or V100)\n",
        "- Hugging Face Token\n",
        "- 12-16 hours training time\n",
        "\n",
        "**Steps:**\n",
        "1. Enable GPU: Runtime → Change runtime type → GPU\n",
        "2. Run all cells in order\n",
        "3. Monitor training progress"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q autotrain-advanced huggingface_hub datasets transformers accelerate bitsandbytes peft"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Login to Hugging Face"
      ],
      "metadata": {
        "id": "login"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Enter your Hugging Face token\n",
        "HF_TOKEN = \"\"  # Replace with your token\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "print(\"Logged in to Hugging Face\")"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Dataset"
      ],
      "metadata": {
        "id": "load_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from Hugging Face Hub\n",
        "dataset = load_dataset(\"dlnkgpt/dlnkgpt-uncensored-dataset\")\n",
        "\n",
        "print(f\"Dataset loaded:\")\n",
        "print(f\"  Train: {len(dataset['train']):,} examples\")\n",
        "print(f\"  Validation: {len(dataset['validation']):,} examples\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample:\")\n",
        "print(dataset['train'][0]['text'][:200] + \"...\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Configure Training"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Training configuration\n",
        "config = {\n",
        "    \"project_name\": \"dlnkgpt-uncensored\",\n",
        "    \"model\": \"EleutherAI/gpt-j-6b\",\n",
        "    \"dataset\": \"dlnkgpt/dlnkgpt-uncensored-dataset\",\n",
        "    \"text_column\": \"text\",\n",
        "    \"epochs\": 3,\n",
        "    \"batch_size\": 4,\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \"gradient_accumulation\": 8,\n",
        "    \"block_size\": 512,\n",
        "    \"use_peft\": True,\n",
        "    \"lora_r\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"fp16\": True,\n",
        "    \"push_to_hub\": True\n",
        "}\n",
        "\n",
        "# Set environment variable\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "\n",
        "print(\"Configuration set\")\n",
        "print(f\"  Base Model: {config['model']}\")\n",
        "print(f\"  Epochs: {config['epochs']}\")\n",
        "print(f\"  Batch Size: {config['batch_size']}\")\n",
        "print(f\"  Learning Rate: {config['learning_rate']}\")\n",
        "print(f\"  PEFT/LoRA: {config['use_peft']}\")"
      ],
      "metadata": {
        "id": "configure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Start Training\n",
        "\n",
        "**Note:** This will take 12-16 hours on T4 GPU."
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset locally first\n",
        "dataset.save_to_disk(\"./autotrain_data\")\n",
        "print(\"Dataset saved locally\")"
      ],
      "metadata": {
        "id": "save_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch AutoTrain\n",
        "!autotrain llm \\\n",
        "  --train \\\n",
        "  --project-name dlnkgpt-uncensored \\\n",
        "  --model EleutherAI/gpt-j-6b \\\n",
        "  --data-path ./autotrain_data \\\n",
        "  --text-column text \\\n",
        "  --lr 2e-5 \\\n",
        "  --epochs 3 \\\n",
        "  --batch-size 4 \\\n",
        "  --warmup-ratio 0.1 \\\n",
        "  --gradient-accumulation 8 \\\n",
        "  --block_size 512 \\\n",
        "  --logging-steps 100 \\\n",
        "  --eval-strategy steps \\\n",
        "  --save-total-limit 2 \\\n",
        "  --peft \\\n",
        "  --lora-r 16 \\\n",
        "  --lora-alpha 32 \\\n",
        "  --lora-dropout 0.05 \\\n",
        "  --mixed-precision fp16 \\\n",
        "  --push-to-hub \\\n",
        "  --username dlnkgpt \\\n",
        "  --token $HF_TOKEN"
      ],
      "metadata": {
        "id": "start_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Test Trained Model"
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Load base model\n",
        "print(\"Loading base model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-j-6b\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "print(\"Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"dlnkgpt/dlnkgpt-uncensored\"\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dlnkgpt/dlnkgpt-uncensored\")\n",
        "\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "# Test generation\n",
        "def generate_response(prompt, max_length=200):\n",
        "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Explain artificial intelligence\",\n",
        "    \"What is machine learning?\",\n",
        "    \"Describe neural networks\"\n",
        "]\n",
        "\n",
        "print(\"Testing model...\")\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nTest {i}: {prompt}\")\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Response: {response}\")"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "**Training Complete!**\n",
        "\n",
        "Your model has been:\n",
        "- Trained on 54,000 examples\n",
        "- Fine-tuned for 3 epochs\n",
        "- Pushed to Hugging Face Hub\n",
        "- Ready to use\n",
        "\n",
        "**Model URL:** https://huggingface.co/dlnkgpt/dlnkgpt-uncensored"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
