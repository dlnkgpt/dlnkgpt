{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# dLNk GPT Uncensored - AutoTrain on GPU\n",
        "\n",
        "This notebook trains the dLNk GPT uncensored model using Hugging Face AutoTrain with GPU acceleration.\n",
        "\n",
        "**Requirements:**\n",
        "- GPU Runtime (T4, A100, or V100)\n",
        "- Hugging Face Token\n",
        "- 12-16 hours training time\n",
        "\n",
        "**Steps:**\n",
        "1. Enable GPU: Runtime â†’ Change runtime type â†’ GPU\n",
        "2. Run all cells in order\n",
        "3. Monitor training progress"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install AutoTrain and dependencies\n",
        "# Note: Install autotrain-advanced FIRST, then it will install correct versions of dependencies\n",
        "!pip install -q autotrain-advanced\n",
        "print(\"AutoTrain installed successfully\")"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Login to Hugging Face"
      ],
      "metadata": {
        "id": "login"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Enter your Hugging Face token here\n",
        "HF_TOKEN = \"\"  # Paste your token between the quotes\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    print(\"Please enter your Hugging Face token above\")\n",
        "else:\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Successfully logged in to Hugging Face!\")"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Dataset"
      ],
      "metadata": {
        "id": "load_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Loading dataset from Hugging Face Hub...\")\n",
        "dataset = load_dataset(\"dlnkgpt/dlnkgpt-uncensored-dataset\")\n",
        "\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"  Training examples: {len(dataset['train']):,}\")\n",
        "print(f\"  Validation examples: {len(dataset['validation']):,}\")\n",
        "\n",
        "# Show a sample\n",
        "print(f\"\\nSample text:\")\n",
        "print(dataset['train'][0]['text'][:300] + \"...\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare Dataset"
      ],
      "metadata": {
        "id": "prepare"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save dataset to local disk for AutoTrain\n",
        "print(\"Saving dataset to local disk...\")\n",
        "dataset.save_to_disk(\"./autotrain_data\")\n",
        "print(\"Dataset saved successfully!\")"
      ],
      "metadata": {
        "id": "save_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Configure Training"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set HF token as environment variable\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "\n",
        "# Training configuration\n",
        "config = {\n",
        "    'project_name': 'dlnkgpt-uncensored',\n",
        "    'model': 'EleutherAI/gpt-j-6b',\n",
        "    'data_path': './autotrain_data',\n",
        "    'text_column': 'text',\n",
        "    'epochs': 3,\n",
        "    'batch_size': 4,\n",
        "    'learning_rate': 2e-5,\n",
        "    'warmup_ratio': 0.1,\n",
        "    'gradient_accumulation': 8,\n",
        "    'block_size': 512,\n",
        "    'lora_r': 16,\n",
        "    'lora_alpha': 32,\n",
        "    'lora_dropout': 0.05\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Base Model: {config['model']}\")\n",
        "print(f\"  Epochs: {config['epochs']}\")\n",
        "print(f\"  Batch Size: {config['batch_size']}\")\n",
        "print(f\"  Effective Batch Size: {config['batch_size'] * config['gradient_accumulation']}\")\n",
        "print(f\"  Learning Rate: {config['learning_rate']}\")\n",
        "print(f\"  Using LoRA: Yes (r={config['lora_r']}, alpha={config['lora_alpha']})\")\n",
        "print(f\"\\nEstimated training time: 12-16 hours on T4 GPU\")"
      ],
      "metadata": {
        "id": "configure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Start Training\n",
        "\n",
        "**Important Notes:**\n",
        "- Training will take 12-16 hours on T4 GPU\n",
        "- You can close this tab and come back later\n",
        "- The model will be automatically pushed to Hugging Face Hub when complete\n",
        "- Check progress by viewing the output below"
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start AutoTrain\n",
        "!autotrain llm \\\n",
        "  --train \\\n",
        "  --project-name {config['project_name']} \\\n",
        "  --model {config['model']} \\\n",
        "  --data-path {config['data_path']} \\\n",
        "  --text-column {config['text_column']} \\\n",
        "  --lr {config['learning_rate']} \\\n",
        "  --epochs {config['epochs']} \\\n",
        "  --batch-size {config['batch_size']} \\\n",
        "  --warmup-ratio {config['warmup_ratio']} \\\n",
        "  --gradient-accumulation {config['gradient_accumulation']} \\\n",
        "  --block_size {config['block_size']} \\\n",
        "  --logging-steps 100 \\\n",
        "  --eval-strategy steps \\\n",
        "  --save-total-limit 2 \\\n",
        "  --peft \\\n",
        "  --lora-r {config['lora_r']} \\\n",
        "  --lora-alpha {config['lora_alpha']} \\\n",
        "  --lora-dropout {config['lora_dropout']} \\\n",
        "  --mixed-precision fp16 \\\n",
        "  --push-to-hub \\\n",
        "  --username dlnkgpt \\\n",
        "  --token $HF_TOKEN"
      ],
      "metadata": {
        "id": "start_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Test the Trained Model\n",
        "\n",
        "Run this cell after training is complete to test your model."
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "print(\"Loading trained model...\")\n",
        "\n",
        "# Load base model\n",
        "print(\"  Loading base model (GPT-J-6B)...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-j-6b\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "print(\"  Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"dlnkgpt/dlnkgpt-uncensored\"\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"  Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dlnkgpt/dlnkgpt-uncensored\")\n",
        "\n",
        "print(\"\\nModel loaded successfully!\\n\")\n",
        "\n",
        "# Test generation function\n",
        "def generate_response(prompt, max_length=200):\n",
        "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test with sample prompts\n",
        "test_prompts = [\n",
        "    \"Explain what artificial intelligence is\",\n",
        "    \"What is the difference between machine learning and deep learning?\",\n",
        "    \"How do neural networks work?\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Testing Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n[Test {i}] Prompt: {prompt}\")\n",
        "    print(\"-\" * 70)\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Response:\\n{response}\")\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "### Training Complete!\n",
        "\n",
        "Your dLNk GPT uncensored model has been successfully trained!\n",
        "\n",
        "**Model Details:**\n",
        "- Base Model: GPT-J-6B (6 billion parameters)\n",
        "- Training Examples: 54,000\n",
        "- Validation Examples: 6,000\n",
        "- Training Method: LoRA/PEFT\n",
        "- Epochs: 3\n",
        "\n",
        "**Model Location:**\n",
        "- Hugging Face Hub: https://huggingface.co/dlnkgpt/dlnkgpt-uncensored\n",
        "- Local: ./dlnkgpt-uncensored/\n",
        "\n",
        "**Next Steps:**\n",
        "1. Test the model with various prompts (see cell above)\n",
        "2. Download the model for local use\n",
        "3. Integrate with your application\n",
        "4. Deploy to production\n",
        "\n",
        "**Using the Model:**\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\")\n",
        "model = PeftModel.from_pretrained(base_model, \"dlnkgpt/dlnkgpt-uncensored\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dlnkgpt/dlnkgpt-uncensored\")\n",
        "```\n",
        "\n",
        "Congratulations! ðŸŽ‰"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
